#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿæˆé™æ€æ•°æ®æ–‡ä»¶ä¾› Cloudflare Pages éƒ¨ç½²ä½¿ç”¨
"""

import json
import os
from pathlib import Path
from workflow_db import WorkflowDatabase

def main():
    """ç”Ÿæˆæ‰€æœ‰é™æ€æ•°æ®æ–‡ä»¶"""
    
    print("ğŸš€ å¼€å§‹ç”Ÿæˆé™æ€æ•°æ®æ–‡ä»¶...")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = Path('static/data')
    data_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… æ•°æ®ç›®å½•: {data_dir}")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db = WorkflowDatabase()
    
    # 1. ç”Ÿæˆæ‰€æœ‰å·¥ä½œæµæ•°æ®
    print("\nğŸ“Š æ­¥éª¤ 1/4: ç”Ÿæˆå·¥ä½œæµæ•°æ®...")
    all_workflows = []
    offset = 0
    limit = 100
    
    while True:
        workflows, total = db.search_workflows(query='', limit=limit, offset=offset)
        if not workflows:
            break
        all_workflows.extend(workflows)
        offset += limit
        print(f"  å·²å¤„ç†: {len(all_workflows)}/{total} ä¸ªå·¥ä½œæµ")
        if offset >= total:
            break
    
    # ä¿å­˜å·¥ä½œæµæ•°æ®
    workflows_file = data_dir / 'workflows.json'
    with open(workflows_file, 'w', encoding='utf-8') as f:
        json.dump({
            'workflows': all_workflows,
            'total': len(all_workflows),
            'generated_at': __import__('datetime').datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… å·²ä¿å­˜ {len(all_workflows)} ä¸ªå·¥ä½œæµåˆ°: {workflows_file}")
    print(f"  ğŸ“¦ æ–‡ä»¶å¤§å°: {workflows_file.stat().st_size / 1024 / 1024:.2f} MB")
    
    # 2. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“ˆ æ­¥éª¤ 2/4: ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
    stats = db.get_stats()
    stats_file = data_dir / 'stats.json'
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… ç»Ÿè®¡ä¿¡æ¯:")
    print(f"     - æ€»å·¥ä½œæµ: {stats['total']}")
    print(f"     - æ´»è·ƒå·¥ä½œæµ: {stats['active']}")
    print(f"     - æ€»èŠ‚ç‚¹æ•°: {stats['total_nodes']}")
    print(f"     - é›†æˆæœåŠ¡: {stats['unique_integrations']}")
    
    # 3. ç”Ÿæˆåˆ†ç±»ä¿¡æ¯
    print("\nğŸ·ï¸  æ­¥éª¤ 3/4: ç”Ÿæˆåˆ†ç±»ä¿¡æ¯...")
    service_categories = db.get_service_categories()
    categories = list(service_categories.keys())
    categories_file = data_dir / 'categories.json'
    with open(categories_file, 'w', encoding='utf-8') as f:
        json.dump({
            'categories': sorted(categories),
            'service_categories': service_categories,
            'total': len(categories)
        }, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… å·²ä¿å­˜ {len(categories)} ä¸ªåˆ†ç±»")
    
    # 4. ç”Ÿæˆé›†æˆæœåŠ¡åˆ—è¡¨
    print("\nğŸ”Œ æ­¥éª¤ 4/4: ç”Ÿæˆé›†æˆæœåŠ¡åˆ—è¡¨...")
    # ä»å·¥ä½œæµä¸­æ”¶é›†æ‰€æœ‰é›†æˆæœåŠ¡
    all_integrations = set()
    for w in all_workflows:
        all_integrations.update(w.get('integrations', []))
    
    integrations_file = data_dir / 'integrations.json'
    with open(integrations_file, 'w', encoding='utf-8') as f:
        json.dump({
            'integrations': sorted(list(all_integrations)),
            'total': len(all_integrations)
        }, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… å·²ä¿å­˜ {len(all_integrations)} ä¸ªé›†æˆæœåŠ¡")
    
    # 5. ç”Ÿæˆæœç´¢ç´¢å¼•ï¼ˆç”¨äºå¿«é€Ÿæœç´¢ï¼‰
    print("\nğŸ” ç”Ÿæˆæœç´¢ç´¢å¼•...")
    search_index = {
        'workflows': [
            {
                'id': w['filename'],
                'name': w['name'],
                'description': w['description'],
                'integrations': w['integrations'],
                'trigger_type': w['trigger_type'],
                'complexity': w['complexity'],
                'active': w.get('active', True),
                'node_count': w.get('node_count', 0)
            }
            for w in all_workflows
        ]
    }
    
    index_file = data_dir / 'search-index.json'
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(search_index, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… å·²ä¿å­˜æœç´¢ç´¢å¼•")
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ‰ é™æ€æ•°æ®ç”Ÿæˆå®Œæˆ!")
    print("=" * 50)
    
    total_size = sum(f.stat().st_size for f in data_dir.glob('*.json'))
    print(f"\nğŸ“¦ ç”Ÿæˆçš„æ–‡ä»¶:")
    for f in sorted(data_dir.glob('*.json')):
        size_mb = f.stat().st_size / 1024 / 1024
        print(f"   - {f.name}: {size_mb:.2f} MB")
    
    print(f"\nğŸ’¾ æ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {data_dir.absolute()}")
    
    print("\nâœ¨ ä¸‹ä¸€æ­¥:")
    print("   1. git add static/data/")
    print("   2. git commit -m 'Add static data for Cloudflare Pages'")
    print("   3. git push")
    print("   4. åœ¨ Cloudflare Pages éƒ¨ç½²ä½ çš„é¡¹ç›®")
    print("\nğŸš€ éƒ¨ç½²åè®¿é—®é€Ÿåº¦å°†éå¸¸å¿«!")

if __name__ == '__main__':
    main()
